{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Problem\r\n",
    "\r\n",
    "You are given data from an Audiobook app. Logically, it relates only to the audio versions of books. Each customer in the database has made a purchase at least once, that's why he/she is in the database. We want to create a machine learning algorithm based on our available data that can predict if a customer will buy again from the Audiobook company.\r\n",
    "\r\n",
    "The main idea is that if a customer has a low probability of coming back, there is no reason to spend any money on advertizing to him/her. If we can focus our efforts ONLY on customers that are likely to convert again, we can make great savings. Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities.\r\n",
    "\r\n",
    "You have a .csv summarizing the data. There are several variables: Customer ID, Book length in mins_avg (average of all purchases), Book length in minutes_sum (sum of all purchases), Price Paid_avg (average of all purchases), Price paid_sum (sum of all purchases), Review (a Boolean variable), Review (out of 10), Total minutes listened, Completion (from 0 to 1), Support requests (number), and Last visited minus purchase date (in days).\r\n",
    "\r\n",
    "So these are the inputs (excluding customer ID, as it is completely arbitrary. It's more like a name, than a number).\r\n",
    "\r\n",
    "The targets are a Boolean variable (so 0, or 1). We are taking a period of 2 years in our inputs, and the next 6 months as targets. So, in fact, we are predicting if: based on the last 2 years of activity and engagement, a customer will convert in the next 6 months. 6 months sounds like a reasonable time. If they don't convert after 6 months, chances are they've gone to a competitor or didn't like the Audiobook way of digesting information. \r\n",
    "\r\n",
    "The task is simple: create a machine learning algorithm, which is able to predict if a customer will buy again. \r\n",
    "\r\n",
    "This is a classification problem with two classes: won't buy and will buy, represented by 0s and 1s. \r\n",
    "\r\n",
    "Good luck!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the machine learning algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import the relevant libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# we must import the libraries once again since we haven't imported them in this file\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets\r\n",
    "npz = np.load('Audiobooks_data_train.npz')\r\n",
    "\r\n",
    "# we extract the inputs using the keyword under which we saved them\r\n",
    "# to ensure that they are all floats, let's also take care of that\r\n",
    "train_inputs = npz['inputs'].astype(np.float)\r\n",
    "# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\r\n",
    "train_targets = npz['targets'].astype(np.int)\r\n",
    "\r\n",
    "# we load the validation data in the temporary variable\r\n",
    "npz = np.load('Audiobooks_data_validation.npz')\r\n",
    "# we can load the inputs and the targets in the same line\r\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\r\n",
    "\r\n",
    "# we load the test data in the temporary variable\r\n",
    "npz = np.load('Audiobooks_data_test.npz')\r\n",
    "# we create 2 variables that will contain the test inputs and the test targets\r\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\r\n",
    "Outline, optimizers, loss, early stopping and training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Optionally set the input size. We won't be using it, but in some cases it is beneficial\r\n",
    "# input_size = 10\r\n",
    "# Set the output size\r\n",
    "output_size = 2\r\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\r\n",
    "hidden_layer_size = 50\r\n",
    "    \r\n",
    "# define how the model will look like\r\n",
    "model = tf.keras.Sequential([\r\n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\r\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\r\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\r\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\r\n",
    "    # the final layer is no different, we just make sure to activate it with softmax\r\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\r\n",
    "])\r\n",
    "\r\n",
    "\r\n",
    "### Choose the optimizer and the loss function\r\n",
    "\r\n",
    "# we define the optimizer we'd like to use, \r\n",
    "# the loss function, \r\n",
    "# and the metrics we are interested in obtaining at each iteration\r\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n",
    "\r\n",
    "### Training\r\n",
    "# That's where we train the model we have built.\r\n",
    "\r\n",
    "# set the batch size\r\n",
    "batch_size = 100\r\n",
    "\r\n",
    "# set a maximum number of training epochs\r\n",
    "max_epochs = 100\r\n",
    "\r\n",
    "# set an early stopping mechanism\r\n",
    "# let's set patience=2, to be a bit tolerant against random validation loss increases\r\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\r\n",
    "\r\n",
    "# fit the model\r\n",
    "# note that this time the train, validation and test data are not iterable\r\n",
    "model.fit(train_inputs, # train inputs\r\n",
    "          train_targets, # train targets\r\n",
    "          batch_size=batch_size, # batch size\r\n",
    "          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\r\n",
    "          # callbacks are functions called by a task when a task is completed\r\n",
    "          # task here is to check if val_loss is increasing\r\n",
    "          callbacks=[early_stopping], # early stopping\r\n",
    "          validation_data=(validation_inputs, validation_targets), # validation data\r\n",
    "          verbose = 2 # making sure we get enough information about the training process\r\n",
    "          )  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 5s - loss: 0.5857 - accuracy: 0.7572 - val_loss: 0.4417 - val_accuracy: 0.8501\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.3801 - accuracy: 0.8745 - val_loss: 0.3276 - val_accuracy: 0.8591\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.3124 - accuracy: 0.8874 - val_loss: 0.3056 - val_accuracy: 0.8725\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.2936 - accuracy: 0.8927 - val_loss: 0.2923 - val_accuracy: 0.8747\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.2807 - accuracy: 0.8949 - val_loss: 0.2812 - val_accuracy: 0.8814\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.2705 - accuracy: 0.8969 - val_loss: 0.2793 - val_accuracy: 0.8814\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.2628 - accuracy: 0.8997 - val_loss: 0.2680 - val_accuracy: 0.8814\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.2563 - accuracy: 0.9028 - val_loss: 0.2654 - val_accuracy: 0.8904\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.2511 - accuracy: 0.9039 - val_loss: 0.2586 - val_accuracy: 0.8949\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.2467 - accuracy: 0.9061 - val_loss: 0.2534 - val_accuracy: 0.8949\n",
      "Epoch 11/100\n",
      "36/36 - 0s - loss: 0.2450 - accuracy: 0.9061 - val_loss: 0.2530 - val_accuracy: 0.8949\n",
      "Epoch 12/100\n",
      "36/36 - 0s - loss: 0.2456 - accuracy: 0.9061 - val_loss: 0.2595 - val_accuracy: 0.8971\n",
      "Epoch 13/100\n",
      "36/36 - 0s - loss: 0.2393 - accuracy: 0.9061 - val_loss: 0.2499 - val_accuracy: 0.8949\n",
      "Epoch 14/100\n",
      "36/36 - 0s - loss: 0.2367 - accuracy: 0.9089 - val_loss: 0.2594 - val_accuracy: 0.8949\n",
      "Epoch 15/100\n",
      "36/36 - 0s - loss: 0.2357 - accuracy: 0.9106 - val_loss: 0.2538 - val_accuracy: 0.8971\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13553e26088>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the model\r\n",
    "\r\n",
    "As we discussed in the lectures, after training on the training data and validating on the validation data, we test the final prediction power of our model by running it on the test dataset that the algorithm has NEVER seen before.\r\n",
    "\r\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset. \r\n",
    "\r\n",
    "The test is the absolute final instance. You should not test before you are completely done with adjusting your model.\r\n",
    "\r\n",
    "If you adjust your model after testing, you will start overfitting the test dataset, which will defeat its purpose."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# declare two variables that are going to contain the two outputs from the evaluate function\r\n",
    "# they are the loss (which is there by default) and whatever was specified in the 'metrics' argument when fitting the model\r\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14/14 [==============================] - 0s 4ms/step - loss: 0.2780 - accuracy: 0.8951\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print('\\nTest loass: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss,test_accuracy*100.))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test loass: 0.28. Test accuracy: 89.51%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Obtain the probability for a customer to convert"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# We can predict the probability of each class using the 'predict' method\r\n",
    "# The output comes in a scientific format\r\n",
    "# Please uncomment the round() method to achive a rounded result (not scientific notation)\r\n",
    "model.predict(test_inputs)#.round(2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[9.76523161e-02, 9.02347744e-01],\n",
       "       [8.98527980e-01, 1.01471975e-01],\n",
       "       [2.48258457e-05, 9.99975204e-01],\n",
       "       [9.75813270e-01, 2.41867248e-02],\n",
       "       [7.02733994e-01, 2.97266006e-01],\n",
       "       [8.60417068e-01, 1.39582902e-01],\n",
       "       [2.10417464e-01, 7.89582491e-01],\n",
       "       [8.91493678e-01, 1.08506352e-01],\n",
       "       [4.65704034e-05, 9.99953389e-01],\n",
       "       [2.49245226e-01, 7.50754774e-01],\n",
       "       [5.25458790e-02, 9.47454095e-01],\n",
       "       [7.66586840e-01, 2.33413115e-01],\n",
       "       [8.24127078e-01, 1.75872967e-01],\n",
       "       [2.16691522e-04, 9.99783337e-01],\n",
       "       [6.53650886e-06, 9.99993443e-01],\n",
       "       [1.51083380e-01, 8.48916650e-01],\n",
       "       [4.56661109e-09, 1.00000000e+00],\n",
       "       [1.49744645e-01, 8.50255311e-01],\n",
       "       [1.56780757e-08, 1.00000000e+00],\n",
       "       [4.43720546e-06, 9.99995589e-01],\n",
       "       [8.79493833e-01, 1.20506130e-01],\n",
       "       [2.01117327e-05, 9.99979854e-01],\n",
       "       [2.08243370e-01, 7.91756690e-01],\n",
       "       [9.28346097e-01, 7.16538429e-02],\n",
       "       [1.77955480e-05, 9.99982238e-01],\n",
       "       [8.97511065e-01, 1.02488935e-01],\n",
       "       [1.51654065e-01, 8.48345876e-01],\n",
       "       [9.18984890e-01, 8.10150579e-02],\n",
       "       [1.09451435e-06, 9.99998927e-01],\n",
       "       [9.22632635e-01, 7.73673877e-02],\n",
       "       [9.99479949e-01, 5.20031492e-04],\n",
       "       [9.55260456e-01, 4.47395481e-02],\n",
       "       [1.36356875e-01, 8.63643110e-01],\n",
       "       [9.17130351e-01, 8.28696489e-02],\n",
       "       [9.63241041e-01, 3.67589295e-02],\n",
       "       [4.27840532e-05, 9.99957204e-01],\n",
       "       [7.48231588e-03, 9.92517710e-01],\n",
       "       [2.60914981e-01, 7.39085078e-01],\n",
       "       [2.75999904e-01, 7.24000156e-01],\n",
       "       [9.33821917e-01, 6.61781356e-02],\n",
       "       [1.34407520e-01, 8.65592480e-01],\n",
       "       [4.07160819e-01, 5.92839181e-01],\n",
       "       [8.89408290e-01, 1.10591754e-01],\n",
       "       [6.39185727e-01, 3.60814333e-01],\n",
       "       [8.93039823e-01, 1.06960163e-01],\n",
       "       [1.00000000e+00, 3.18151430e-13],\n",
       "       [9.23100948e-01, 7.68989772e-02],\n",
       "       [1.68183491e-01, 8.31816494e-01],\n",
       "       [1.45443395e-01, 8.54556620e-01],\n",
       "       [1.62686065e-01, 8.37313950e-01],\n",
       "       [9.22632635e-01, 7.73673877e-02],\n",
       "       [2.18211561e-01, 7.81788468e-01],\n",
       "       [1.14886597e-01, 8.85113358e-01],\n",
       "       [1.26677170e-01, 8.73322845e-01],\n",
       "       [2.07875576e-03, 9.97921288e-01],\n",
       "       [8.43573988e-10, 1.00000000e+00],\n",
       "       [9.28346097e-01, 7.16538429e-02],\n",
       "       [8.34073067e-01, 1.65926918e-01],\n",
       "       [9.41155255e-01, 5.88447750e-02],\n",
       "       [9.09639299e-01, 9.03607309e-02],\n",
       "       [2.60532298e-03, 9.97394681e-01],\n",
       "       [9.28197682e-01, 7.18023479e-02],\n",
       "       [6.32187068e-01, 3.67812902e-01],\n",
       "       [8.95187795e-01, 1.04812197e-01],\n",
       "       [2.62132525e-01, 7.37867475e-01],\n",
       "       [1.17151614e-03, 9.98828471e-01],\n",
       "       [9.98656988e-01, 1.34303316e-03],\n",
       "       [9.19721425e-01, 8.02785307e-02],\n",
       "       [9.99895334e-01, 1.04646606e-04],\n",
       "       [2.25994503e-03, 9.97740030e-01],\n",
       "       [9.99999523e-01, 5.27895168e-07],\n",
       "       [1.07340804e-06, 9.99998927e-01],\n",
       "       [8.98264170e-01, 1.01735838e-01],\n",
       "       [1.48017043e-10, 1.00000000e+00],\n",
       "       [8.68627429e-01, 1.31372556e-01],\n",
       "       [2.53713936e-01, 7.46286035e-01],\n",
       "       [1.04623159e-11, 1.00000000e+00],\n",
       "       [3.31824740e-05, 9.99966860e-01],\n",
       "       [8.98885906e-01, 1.01114154e-01],\n",
       "       [7.36320689e-02, 9.26367939e-01],\n",
       "       [1.49744645e-01, 8.50255311e-01],\n",
       "       [3.93899009e-02, 9.60610032e-01],\n",
       "       [2.53713936e-01, 7.46286035e-01],\n",
       "       [1.51721418e-01, 8.48278582e-01],\n",
       "       [8.29293907e-01, 1.70706064e-01],\n",
       "       [1.00000000e+00, 1.81375373e-10],\n",
       "       [9.52510118e-01, 4.74899150e-02],\n",
       "       [1.26823962e-01, 8.73175979e-01],\n",
       "       [9.99997973e-01, 2.05321680e-06],\n",
       "       [1.50526017e-01, 8.49473953e-01],\n",
       "       [1.00000000e+00, 1.70038863e-08],\n",
       "       [1.00000000e+00, 6.23112881e-11],\n",
       "       [1.25947714e-01, 8.74052227e-01],\n",
       "       [9.98168111e-01, 1.83187274e-03],\n",
       "       [8.86981264e-02, 9.11301851e-01],\n",
       "       [1.45459011e-01, 8.54541004e-01],\n",
       "       [4.34373856e-01, 5.65626144e-01],\n",
       "       [8.11831057e-02, 9.18816864e-01],\n",
       "       [8.41185987e-01, 1.58813983e-01],\n",
       "       [1.00000000e+00, 2.24052110e-09],\n",
       "       [9.48817551e-01, 5.11824265e-02],\n",
       "       [7.44272649e-01, 2.55727321e-01],\n",
       "       [1.44484028e-01, 8.55515957e-01],\n",
       "       [9.19733822e-01, 8.02662149e-02],\n",
       "       [1.20025232e-01, 8.79974782e-01],\n",
       "       [2.33824804e-01, 7.66175210e-01],\n",
       "       [2.65214175e-01, 7.34785855e-01],\n",
       "       [5.16912878e-01, 4.83087093e-01],\n",
       "       [8.99733603e-01, 1.00266412e-01],\n",
       "       [9.99901652e-01, 9.83292339e-05],\n",
       "       [9.33076441e-01, 6.69236034e-02],\n",
       "       [8.68997037e-01, 1.31002992e-01],\n",
       "       [1.00000000e+00, 2.71766377e-12],\n",
       "       [1.47223594e-10, 1.00000000e+00],\n",
       "       [8.51118267e-02, 9.14888203e-01],\n",
       "       [8.65214691e-02, 9.13478553e-01],\n",
       "       [1.45459011e-01, 8.54541004e-01],\n",
       "       [8.33911598e-01, 1.66088402e-01],\n",
       "       [2.93596517e-02, 9.70640361e-01],\n",
       "       [1.00000000e+00, 8.67187389e-09],\n",
       "       [1.18391141e-01, 8.81608844e-01],\n",
       "       [1.35810196e-01, 8.64189863e-01],\n",
       "       [9.22475010e-02, 9.07752514e-01],\n",
       "       [1.55211434e-01, 8.44788492e-01],\n",
       "       [1.43254876e-01, 8.56745124e-01],\n",
       "       [9.94858444e-02, 9.00514126e-01],\n",
       "       [2.34733980e-05, 9.99976516e-01],\n",
       "       [1.19254351e-01, 8.80745649e-01],\n",
       "       [5.81850810e-03, 9.94181454e-01],\n",
       "       [9.33076441e-01, 6.69236034e-02],\n",
       "       [7.27622330e-01, 2.72377610e-01],\n",
       "       [3.17819387e-01, 6.82180583e-01],\n",
       "       [8.71320605e-01, 1.28679350e-01],\n",
       "       [7.79247060e-02, 9.22075331e-01],\n",
       "       [5.82441270e-01, 4.17558700e-01],\n",
       "       [8.95759523e-01, 1.04240485e-01],\n",
       "       [7.64139056e-01, 2.35860914e-01],\n",
       "       [1.82932745e-08, 1.00000000e+00],\n",
       "       [1.45459011e-01, 8.54541004e-01],\n",
       "       [1.00000000e+00, 4.32454517e-09],\n",
       "       [9.99994397e-01, 5.61291336e-06],\n",
       "       [9.99707282e-01, 2.92720739e-04],\n",
       "       [1.00000000e+00, 6.46946408e-09],\n",
       "       [2.53713936e-01, 7.46286035e-01],\n",
       "       [9.93296683e-01, 6.70333905e-03],\n",
       "       [9.34787273e-01, 6.52127415e-02],\n",
       "       [1.00000000e+00, 3.93704569e-10],\n",
       "       [9.97887909e-01, 2.11214763e-03],\n",
       "       [2.31303420e-05, 9.99976873e-01],\n",
       "       [1.00000000e+00, 1.01619546e-11],\n",
       "       [2.05385253e-01, 7.94614732e-01],\n",
       "       [8.51431131e-01, 1.48568884e-01],\n",
       "       [8.96426797e-01, 1.03573225e-01],\n",
       "       [9.82733727e-01, 1.72663312e-02],\n",
       "       [9.99999166e-01, 8.72533235e-07],\n",
       "       [9.00693417e-01, 9.93066207e-02],\n",
       "       [8.11618194e-03, 9.91883755e-01],\n",
       "       [9.68320489e-01, 3.16794775e-02],\n",
       "       [9.89539325e-01, 1.04606366e-02],\n",
       "       [9.35586512e-01, 6.44134581e-02],\n",
       "       [8.40626732e-02, 9.15937304e-01],\n",
       "       [3.43179345e-01, 6.56820714e-01],\n",
       "       [9.32314098e-01, 6.76859096e-02],\n",
       "       [8.98281574e-01, 1.01718374e-01],\n",
       "       [2.67394364e-01, 7.32605577e-01],\n",
       "       [9.63241041e-01, 3.67589295e-02],\n",
       "       [1.63686007e-01, 8.36313963e-01],\n",
       "       [6.93281181e-04, 9.99306679e-01],\n",
       "       [9.52801543e-07, 9.99999046e-01],\n",
       "       [9.41155255e-01, 5.88447750e-02],\n",
       "       [8.73391628e-01, 1.26608342e-01],\n",
       "       [7.28792071e-01, 2.71207899e-01],\n",
       "       [5.60988730e-04, 9.99438941e-01],\n",
       "       [6.04830742e-01, 3.95169258e-01],\n",
       "       [8.23615253e-01, 1.76384732e-01],\n",
       "       [2.37128489e-07, 9.99999762e-01],\n",
       "       [8.50277126e-01, 1.49722859e-01],\n",
       "       [9.16397274e-01, 8.36027414e-02],\n",
       "       [1.45328596e-01, 8.54671359e-01],\n",
       "       [9.64885235e-01, 3.51148099e-02],\n",
       "       [2.78045740e-02, 9.72195446e-01],\n",
       "       [1.44851834e-01, 8.55148196e-01],\n",
       "       [1.69372726e-02, 9.83062744e-01],\n",
       "       [1.50526017e-01, 8.49473953e-01],\n",
       "       [8.11505616e-01, 1.88494414e-01],\n",
       "       [4.43882108e-01, 5.56117892e-01],\n",
       "       [9.99990702e-01, 9.24209235e-06],\n",
       "       [1.46360606e-01, 8.53639364e-01],\n",
       "       [1.79817912e-03, 9.98201847e-01],\n",
       "       [8.74811172e-01, 1.25188813e-01],\n",
       "       [1.45328581e-01, 8.54671478e-01],\n",
       "       [4.93171722e-01, 5.06828249e-01],\n",
       "       [9.57296014e-01, 4.27040048e-02],\n",
       "       [8.33142996e-01, 1.66856945e-01],\n",
       "       [1.86297519e-04, 9.99813735e-01],\n",
       "       [8.36770684e-02, 9.16322887e-01],\n",
       "       [7.21226633e-01, 2.78773367e-01],\n",
       "       [2.76253813e-05, 9.99972343e-01],\n",
       "       [9.99700189e-01, 2.99876148e-04],\n",
       "       [6.08873367e-01, 3.91126633e-01],\n",
       "       [2.34063060e-04, 9.99765933e-01],\n",
       "       [8.54281962e-01, 1.45717993e-01],\n",
       "       [2.66282052e-01, 7.33717978e-01],\n",
       "       [9.99999762e-01, 2.25733771e-07],\n",
       "       [1.24204991e-04, 9.99875784e-01],\n",
       "       [1.82662344e-10, 1.00000000e+00],\n",
       "       [9.97156858e-01, 2.84319371e-03],\n",
       "       [1.04374588e-01, 8.95625472e-01],\n",
       "       [3.38962818e-05, 9.99966145e-01],\n",
       "       [1.10921718e-01, 8.89078259e-01],\n",
       "       [9.99999881e-01, 9.51404431e-08],\n",
       "       [7.56905019e-01, 2.43094981e-01],\n",
       "       [7.69819319e-01, 2.30180651e-01],\n",
       "       [5.01998007e-01, 4.98001933e-01],\n",
       "       [6.91595487e-05, 9.99930859e-01],\n",
       "       [9.26607490e-01, 7.33925626e-02],\n",
       "       [1.00000000e+00, 2.34440418e-12],\n",
       "       [9.70851004e-01, 2.91490369e-02],\n",
       "       [8.91038775e-01, 1.08961202e-01],\n",
       "       [1.04497395e-01, 8.95502567e-01],\n",
       "       [9.25043583e-01, 7.49563873e-02],\n",
       "       [2.76157051e-01, 7.23842978e-01],\n",
       "       [2.51790434e-01, 7.48209596e-01],\n",
       "       [1.28570769e-04, 9.99871373e-01],\n",
       "       [1.72328278e-01, 8.27671766e-01],\n",
       "       [2.38607511e-01, 7.61392474e-01],\n",
       "       [1.26139119e-01, 8.73860836e-01],\n",
       "       [7.61118889e-01, 2.38881081e-01],\n",
       "       [2.44345590e-01, 7.55654395e-01],\n",
       "       [9.99999762e-01, 2.08375369e-07],\n",
       "       [9.99998450e-01, 1.55616090e-06],\n",
       "       [7.98327148e-01, 2.01672792e-01],\n",
       "       [8.28237474e-01, 1.71762541e-01],\n",
       "       [6.39813766e-03, 9.93601859e-01],\n",
       "       [1.50526017e-01, 8.49473953e-01],\n",
       "       [1.00000000e+00, 2.43872695e-14],\n",
       "       [1.60297647e-01, 8.39702368e-01],\n",
       "       [1.56154545e-06, 9.99998450e-01],\n",
       "       [6.73726201e-01, 3.26273799e-01],\n",
       "       [9.97343361e-01, 2.65664514e-03],\n",
       "       [1.52326174e-05, 9.99984741e-01],\n",
       "       [3.26282352e-01, 6.73717678e-01],\n",
       "       [9.96991038e-01, 3.00899846e-03],\n",
       "       [1.64435416e-01, 8.35564554e-01],\n",
       "       [1.08412012e-01, 8.91587973e-01],\n",
       "       [5.95814083e-04, 9.99404192e-01],\n",
       "       [9.25493956e-01, 7.45060295e-02],\n",
       "       [7.88302779e-01, 2.11697266e-01],\n",
       "       [9.16586637e-01, 8.34133700e-02],\n",
       "       [8.45545232e-01, 1.54454723e-01],\n",
       "       [1.00000000e+00, 5.16266683e-14],\n",
       "       [9.99999762e-01, 2.05696082e-07],\n",
       "       [1.00000000e+00, 2.54502582e-12],\n",
       "       [1.84348563e-03, 9.98156488e-01],\n",
       "       [5.32707607e-04, 9.99467313e-01],\n",
       "       [1.20944038e-01, 8.79055977e-01],\n",
       "       [7.96163513e-05, 9.99920368e-01],\n",
       "       [2.36412380e-02, 9.76358771e-01],\n",
       "       [2.06317054e-03, 9.97936845e-01],\n",
       "       [3.34857047e-01, 6.65142953e-01],\n",
       "       [1.62141241e-05, 9.99983788e-01],\n",
       "       [9.07424450e-01, 9.25756022e-02],\n",
       "       [9.57216382e-01, 4.27835621e-02],\n",
       "       [6.43368840e-01, 3.56631249e-01],\n",
       "       [6.90772868e-05, 9.99930859e-01],\n",
       "       [8.34236860e-01, 1.65763140e-01],\n",
       "       [1.00000000e+00, 6.73536476e-14],\n",
       "       [8.93204808e-01, 1.06795140e-01],\n",
       "       [9.99490142e-01, 5.09815000e-04],\n",
       "       [1.75904155e-01, 8.24095905e-01],\n",
       "       [9.99999642e-01, 3.47794895e-07],\n",
       "       [1.38034433e-01, 8.61965597e-01],\n",
       "       [9.24474835e-01, 7.55251572e-02],\n",
       "       [1.25793576e-01, 8.74206424e-01],\n",
       "       [2.53713936e-01, 7.46286035e-01],\n",
       "       [9.43912119e-02, 9.05608773e-01],\n",
       "       [1.01302400e-01, 8.98697615e-01],\n",
       "       [1.32847369e-01, 8.67152631e-01],\n",
       "       [7.03415573e-01, 2.96584368e-01],\n",
       "       [8.37956369e-01, 1.62043571e-01],\n",
       "       [9.89184976e-01, 1.08150868e-02],\n",
       "       [8.16097975e-01, 1.83902085e-01],\n",
       "       [7.30069205e-02, 9.26993072e-01],\n",
       "       [9.63595986e-01, 3.64040360e-02],\n",
       "       [1.91466436e-02, 9.80853319e-01],\n",
       "       [9.30682182e-01, 6.93178028e-02],\n",
       "       [1.86848979e-06, 9.99998093e-01],\n",
       "       [2.26350203e-01, 7.73649871e-01],\n",
       "       [9.89492059e-01, 1.05079841e-02],\n",
       "       [1.00000000e+00, 4.97072108e-08],\n",
       "       [8.03907931e-01, 1.96092084e-01],\n",
       "       [1.12959884e-01, 8.87040138e-01],\n",
       "       [9.42617133e-02, 9.05738235e-01],\n",
       "       [6.66521737e-05, 9.99933362e-01],\n",
       "       [9.22937334e-01, 7.70626962e-02],\n",
       "       [2.33824804e-01, 7.66175210e-01],\n",
       "       [1.23569183e-01, 8.76430869e-01],\n",
       "       [8.04333638e-07, 9.99999166e-01],\n",
       "       [9.99998212e-01, 1.78340849e-06],\n",
       "       [1.51501223e-01, 8.48498821e-01],\n",
       "       [1.34797785e-02, 9.86520171e-01],\n",
       "       [1.00000000e+00, 4.94879268e-12],\n",
       "       [1.69638172e-01, 8.30361903e-01],\n",
       "       [8.81978452e-01, 1.18021585e-01],\n",
       "       [1.38488173e-01, 8.61511827e-01],\n",
       "       [6.09112620e-01, 3.90887409e-01],\n",
       "       [2.59141535e-01, 7.40858436e-01],\n",
       "       [1.60798535e-01, 8.39201450e-01],\n",
       "       [9.94501703e-11, 1.00000000e+00],\n",
       "       [1.00000000e+00, 8.25165047e-10],\n",
       "       [9.11086917e-01, 8.89130235e-02],\n",
       "       [3.55574947e-08, 1.00000000e+00],\n",
       "       [6.30738437e-01, 3.69261563e-01],\n",
       "       [8.64946306e-01, 1.35053739e-01],\n",
       "       [9.99917746e-01, 8.22202346e-05],\n",
       "       [1.45459011e-01, 8.54541004e-01],\n",
       "       [8.00072551e-01, 1.99927464e-01],\n",
       "       [2.21824594e-05, 9.99977827e-01],\n",
       "       [1.45459011e-01, 8.54541004e-01],\n",
       "       [1.24503627e-01, 8.75496387e-01],\n",
       "       [9.34968830e-06, 9.99990702e-01],\n",
       "       [7.00571225e-04, 9.99299407e-01],\n",
       "       [1.00000000e+00, 3.19732529e-09],\n",
       "       [7.22945452e-01, 2.77054578e-01],\n",
       "       [1.00000000e+00, 1.17612300e-12],\n",
       "       [1.49032950e-01, 8.50967109e-01],\n",
       "       [6.83403134e-01, 3.16596925e-01],\n",
       "       [9.00315523e-01, 9.96844694e-02],\n",
       "       [1.46562979e-01, 8.53437006e-01],\n",
       "       [8.04509036e-03, 9.91954982e-01],\n",
       "       [8.69731367e-01, 1.30268618e-01],\n",
       "       [8.44109118e-01, 1.55890882e-01],\n",
       "       [7.10336506e-01, 2.89663553e-01],\n",
       "       [1.00000000e+00, 7.38221712e-13],\n",
       "       [4.49154824e-02, 9.55084562e-01],\n",
       "       [9.99998450e-01, 1.51097788e-06],\n",
       "       [1.50526017e-01, 8.49473953e-01],\n",
       "       [1.26085162e-01, 8.73914838e-01],\n",
       "       [2.75999904e-01, 7.24000156e-01],\n",
       "       [1.09557673e-01, 8.90442371e-01],\n",
       "       [2.53713936e-01, 7.46286035e-01],\n",
       "       [9.30729985e-01, 6.92700148e-02],\n",
       "       [1.36595974e-02, 9.86340404e-01],\n",
       "       [1.00000000e+00, 1.74780423e-09],\n",
       "       [7.18082249e-01, 2.81917810e-01],\n",
       "       [9.82289076e-01, 1.77109316e-02],\n",
       "       [8.77767146e-01, 1.22232862e-01],\n",
       "       [3.60783339e-02, 9.63921607e-01],\n",
       "       [8.71579170e-01, 1.28420785e-01],\n",
       "       [1.09494373e-01, 8.90505672e-01],\n",
       "       [1.75179914e-02, 9.82481956e-01],\n",
       "       [9.99347508e-01, 6.52446062e-04],\n",
       "       [9.57114398e-01, 4.28856425e-02],\n",
       "       [1.23584531e-02, 9.87641573e-01],\n",
       "       [1.08130820e-01, 8.91869247e-01],\n",
       "       [9.99966145e-01, 3.38335267e-05],\n",
       "       [9.09389913e-01, 9.06101093e-02],\n",
       "       [1.08503409e-01, 8.91496658e-01],\n",
       "       [1.31385654e-01, 8.68614316e-01],\n",
       "       [1.00000000e+00, 9.52794565e-14],\n",
       "       [9.75754678e-01, 2.42452696e-02],\n",
       "       [9.99999762e-01, 1.98062907e-07],\n",
       "       [2.34562635e-01, 7.65437365e-01],\n",
       "       [1.79370873e-05, 9.99982119e-01],\n",
       "       [9.71399620e-02, 9.02860045e-01],\n",
       "       [1.45459011e-01, 8.54541004e-01],\n",
       "       [8.54909062e-01, 1.45090923e-01],\n",
       "       [1.67423976e-04, 9.99832511e-01],\n",
       "       [1.45562738e-01, 8.54437292e-01],\n",
       "       [8.40027584e-04, 9.99159932e-01],\n",
       "       [9.23929691e-01, 7.60702863e-02],\n",
       "       [9.30729985e-01, 6.92700148e-02],\n",
       "       [9.97938097e-01, 2.06188182e-03],\n",
       "       [1.15672738e-09, 1.00000000e+00],\n",
       "       [8.86945844e-01, 1.13054097e-01],\n",
       "       [1.01578468e-02, 9.89842117e-01],\n",
       "       [1.07520930e-01, 8.92479062e-01],\n",
       "       [1.45630181e-01, 8.54369819e-01],\n",
       "       [1.49293795e-01, 8.50706220e-01],\n",
       "       [9.99128401e-01, 8.71608092e-04],\n",
       "       [9.99991417e-01, 8.58071144e-06],\n",
       "       [1.27627300e-05, 9.99987245e-01],\n",
       "       [9.16397274e-01, 8.36027265e-02],\n",
       "       [1.50525987e-01, 8.49473953e-01],\n",
       "       [1.45459011e-01, 8.54541004e-01],\n",
       "       [8.54246020e-02, 9.14575398e-01],\n",
       "       [2.64986575e-01, 7.35013425e-01],\n",
       "       [1.16601095e-09, 1.00000000e+00],\n",
       "       [2.18939334e-01, 7.81060696e-01],\n",
       "       [1.08693793e-01, 8.91306221e-01],\n",
       "       [9.99955893e-01, 4.41626144e-05],\n",
       "       [2.53713936e-01, 7.46286035e-01],\n",
       "       [7.49697626e-01, 2.50302345e-01],\n",
       "       [1.00000000e+00, 3.94822786e-10],\n",
       "       [3.12332062e-08, 1.00000000e+00],\n",
       "       [1.38727828e-05, 9.99986172e-01],\n",
       "       [4.83905375e-01, 5.16094625e-01],\n",
       "       [9.17422712e-01, 8.25772360e-02],\n",
       "       [8.44801724e-01, 1.55198276e-01],\n",
       "       [1.46046191e-01, 8.53953838e-01],\n",
       "       [1.67640880e-01, 8.32359076e-01],\n",
       "       [1.39828131e-01, 8.60171914e-01],\n",
       "       [9.04054701e-01, 9.59452540e-02],\n",
       "       [6.59294333e-03, 9.93407011e-01],\n",
       "       [1.80177718e-01, 8.19822252e-01],\n",
       "       [1.48018479e-01, 8.51981461e-01],\n",
       "       [1.13199055e-01, 8.86800885e-01],\n",
       "       [2.50311978e-02, 9.74968791e-01],\n",
       "       [9.93227184e-01, 6.77286508e-03],\n",
       "       [1.09650366e-01, 8.90349627e-01],\n",
       "       [9.18080270e-01, 8.19197372e-02],\n",
       "       [3.04815549e-05, 9.99969482e-01],\n",
       "       [1.80611178e-01, 8.19388866e-01],\n",
       "       [1.50526017e-01, 8.49473953e-01],\n",
       "       [7.90403843e-01, 2.09596127e-01],\n",
       "       [8.89646053e-01, 1.10353947e-01],\n",
       "       [1.15661705e-05, 9.99988437e-01],\n",
       "       [2.00325121e-06, 9.99997973e-01],\n",
       "       [9.98554766e-01, 1.44520437e-03],\n",
       "       [2.58626729e-01, 7.41373301e-01],\n",
       "       [9.46783423e-01, 5.32166101e-02],\n",
       "       [4.73875195e-01, 5.26124775e-01],\n",
       "       [8.31471682e-01, 1.68528318e-01],\n",
       "       [6.28215551e-01, 3.71784449e-01],\n",
       "       [1.61879808e-02, 9.83812034e-01],\n",
       "       [9.99530196e-01, 4.69867897e-04],\n",
       "       [9.86659288e-01, 1.33407293e-02],\n",
       "       [9.28505301e-01, 7.14946836e-02],\n",
       "       [9.41742718e-01, 5.82572445e-02],\n",
       "       [9.29162860e-01, 7.08370954e-02],\n",
       "       [1.00000000e+00, 3.34499637e-16],\n",
       "       [9.61574316e-01, 3.84256691e-02],\n",
       "       [2.76157051e-01, 7.23842978e-01],\n",
       "       [4.91212755e-02, 9.50878680e-01],\n",
       "       [9.99998569e-01, 1.43368891e-06],\n",
       "       [1.50526017e-01, 8.49473953e-01],\n",
       "       [1.02904841e-01, 8.97095203e-01],\n",
       "       [8.44811723e-02, 9.15518820e-01],\n",
       "       [9.99459684e-01, 5.40266919e-04],\n",
       "       [8.73404671e-04, 9.99126613e-01],\n",
       "       [8.91493678e-01, 1.08506352e-01],\n",
       "       [1.11608937e-01, 8.88391078e-01],\n",
       "       [1.03181489e-01, 8.96818519e-01],\n",
       "       [1.19606622e-01, 8.80393386e-01],\n",
       "       [1.00000000e+00, 3.48093954e-09],\n",
       "       [3.38962818e-05, 9.99966145e-01],\n",
       "       [3.52848438e-05, 9.99964714e-01],\n",
       "       [3.21192443e-01, 6.78807557e-01]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model.predict(test_inputs).round(2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.1 , 0.9 ],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.98, 0.02],\n",
       "       [0.7 , 0.3 ],\n",
       "       [0.86, 0.14],\n",
       "       [0.21, 0.79],\n",
       "       [0.89, 0.11],\n",
       "       [0.  , 1.  ],\n",
       "       [0.25, 0.75],\n",
       "       [0.05, 0.95],\n",
       "       [0.77, 0.23],\n",
       "       [0.82, 0.18],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.15, 0.85],\n",
       "       [0.  , 1.  ],\n",
       "       [0.15, 0.85],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.88, 0.12],\n",
       "       [0.  , 1.  ],\n",
       "       [0.21, 0.79],\n",
       "       [0.93, 0.07],\n",
       "       [0.  , 1.  ],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.15, 0.85],\n",
       "       [0.92, 0.08],\n",
       "       [0.  , 1.  ],\n",
       "       [0.92, 0.08],\n",
       "       [1.  , 0.  ],\n",
       "       [0.96, 0.04],\n",
       "       [0.14, 0.86],\n",
       "       [0.92, 0.08],\n",
       "       [0.96, 0.04],\n",
       "       [0.  , 1.  ],\n",
       "       [0.01, 0.99],\n",
       "       [0.26, 0.74],\n",
       "       [0.28, 0.72],\n",
       "       [0.93, 0.07],\n",
       "       [0.13, 0.87],\n",
       "       [0.41, 0.59],\n",
       "       [0.89, 0.11],\n",
       "       [0.64, 0.36],\n",
       "       [0.89, 0.11],\n",
       "       [1.  , 0.  ],\n",
       "       [0.92, 0.08],\n",
       "       [0.17, 0.83],\n",
       "       [0.15, 0.85],\n",
       "       [0.16, 0.84],\n",
       "       [0.92, 0.08],\n",
       "       [0.22, 0.78],\n",
       "       [0.11, 0.89],\n",
       "       [0.13, 0.87],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.93, 0.07],\n",
       "       [0.83, 0.17],\n",
       "       [0.94, 0.06],\n",
       "       [0.91, 0.09],\n",
       "       [0.  , 1.  ],\n",
       "       [0.93, 0.07],\n",
       "       [0.63, 0.37],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.26, 0.74],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.92, 0.08],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.87, 0.13],\n",
       "       [0.25, 0.75],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.07, 0.93],\n",
       "       [0.15, 0.85],\n",
       "       [0.04, 0.96],\n",
       "       [0.25, 0.75],\n",
       "       [0.15, 0.85],\n",
       "       [0.83, 0.17],\n",
       "       [1.  , 0.  ],\n",
       "       [0.95, 0.05],\n",
       "       [0.13, 0.87],\n",
       "       [1.  , 0.  ],\n",
       "       [0.15, 0.85],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.13, 0.87],\n",
       "       [1.  , 0.  ],\n",
       "       [0.09, 0.91],\n",
       "       [0.15, 0.85],\n",
       "       [0.43, 0.57],\n",
       "       [0.08, 0.92],\n",
       "       [0.84, 0.16],\n",
       "       [1.  , 0.  ],\n",
       "       [0.95, 0.05],\n",
       "       [0.74, 0.26],\n",
       "       [0.14, 0.86],\n",
       "       [0.92, 0.08],\n",
       "       [0.12, 0.88],\n",
       "       [0.23, 0.77],\n",
       "       [0.27, 0.73],\n",
       "       [0.52, 0.48],\n",
       "       [0.9 , 0.1 ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.93, 0.07],\n",
       "       [0.87, 0.13],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.09, 0.91],\n",
       "       [0.09, 0.91],\n",
       "       [0.15, 0.85],\n",
       "       [0.83, 0.17],\n",
       "       [0.03, 0.97],\n",
       "       [1.  , 0.  ],\n",
       "       [0.12, 0.88],\n",
       "       [0.14, 0.86],\n",
       "       [0.09, 0.91],\n",
       "       [0.16, 0.84],\n",
       "       [0.14, 0.86],\n",
       "       [0.1 , 0.9 ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.12, 0.88],\n",
       "       [0.01, 0.99],\n",
       "       [0.93, 0.07],\n",
       "       [0.73, 0.27],\n",
       "       [0.32, 0.68],\n",
       "       [0.87, 0.13],\n",
       "       [0.08, 0.92],\n",
       "       [0.58, 0.42],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.76, 0.24],\n",
       "       [0.  , 1.  ],\n",
       "       [0.15, 0.85],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.25, 0.75],\n",
       "       [0.99, 0.01],\n",
       "       [0.93, 0.07],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.21, 0.79],\n",
       "       [0.85, 0.15],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.98, 0.02],\n",
       "       [1.  , 0.  ],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.01, 0.99],\n",
       "       [0.97, 0.03],\n",
       "       [0.99, 0.01],\n",
       "       [0.94, 0.06],\n",
       "       [0.08, 0.92],\n",
       "       [0.34, 0.66],\n",
       "       [0.93, 0.07],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.27, 0.73],\n",
       "       [0.96, 0.04],\n",
       "       [0.16, 0.84],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.94, 0.06],\n",
       "       [0.87, 0.13],\n",
       "       [0.73, 0.27],\n",
       "       [0.  , 1.  ],\n",
       "       [0.6 , 0.4 ],\n",
       "       [0.82, 0.18],\n",
       "       [0.  , 1.  ],\n",
       "       [0.85, 0.15],\n",
       "       [0.92, 0.08],\n",
       "       [0.15, 0.85],\n",
       "       [0.96, 0.04],\n",
       "       [0.03, 0.97],\n",
       "       [0.14, 0.86],\n",
       "       [0.02, 0.98],\n",
       "       [0.15, 0.85],\n",
       "       [0.81, 0.19],\n",
       "       [0.44, 0.56],\n",
       "       [1.  , 0.  ],\n",
       "       [0.15, 0.85],\n",
       "       [0.  , 1.  ],\n",
       "       [0.87, 0.13],\n",
       "       [0.15, 0.85],\n",
       "       [0.49, 0.51],\n",
       "       [0.96, 0.04],\n",
       "       [0.83, 0.17],\n",
       "       [0.  , 1.  ],\n",
       "       [0.08, 0.92],\n",
       "       [0.72, 0.28],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.61, 0.39],\n",
       "       [0.  , 1.  ],\n",
       "       [0.85, 0.15],\n",
       "       [0.27, 0.73],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.1 , 0.9 ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.11, 0.89],\n",
       "       [1.  , 0.  ],\n",
       "       [0.76, 0.24],\n",
       "       [0.77, 0.23],\n",
       "       [0.5 , 0.5 ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.93, 0.07],\n",
       "       [1.  , 0.  ],\n",
       "       [0.97, 0.03],\n",
       "       [0.89, 0.11],\n",
       "       [0.1 , 0.9 ],\n",
       "       [0.93, 0.07],\n",
       "       [0.28, 0.72],\n",
       "       [0.25, 0.75],\n",
       "       [0.  , 1.  ],\n",
       "       [0.17, 0.83],\n",
       "       [0.24, 0.76],\n",
       "       [0.13, 0.87],\n",
       "       [0.76, 0.24],\n",
       "       [0.24, 0.76],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.8 , 0.2 ],\n",
       "       [0.83, 0.17],\n",
       "       [0.01, 0.99],\n",
       "       [0.15, 0.85],\n",
       "       [1.  , 0.  ],\n",
       "       [0.16, 0.84],\n",
       "       [0.  , 1.  ],\n",
       "       [0.67, 0.33],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.33, 0.67],\n",
       "       [1.  , 0.  ],\n",
       "       [0.16, 0.84],\n",
       "       [0.11, 0.89],\n",
       "       [0.  , 1.  ],\n",
       "       [0.93, 0.07],\n",
       "       [0.79, 0.21],\n",
       "       [0.92, 0.08],\n",
       "       [0.85, 0.15],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.12, 0.88],\n",
       "       [0.  , 1.  ],\n",
       "       [0.02, 0.98],\n",
       "       [0.  , 1.  ],\n",
       "       [0.33, 0.67],\n",
       "       [0.  , 1.  ],\n",
       "       [0.91, 0.09],\n",
       "       [0.96, 0.04],\n",
       "       [0.64, 0.36],\n",
       "       [0.  , 1.  ],\n",
       "       [0.83, 0.17],\n",
       "       [1.  , 0.  ],\n",
       "       [0.89, 0.11],\n",
       "       [1.  , 0.  ],\n",
       "       [0.18, 0.82],\n",
       "       [1.  , 0.  ],\n",
       "       [0.14, 0.86],\n",
       "       [0.92, 0.08],\n",
       "       [0.13, 0.87],\n",
       "       [0.25, 0.75],\n",
       "       [0.09, 0.91],\n",
       "       [0.1 , 0.9 ],\n",
       "       [0.13, 0.87],\n",
       "       [0.7 , 0.3 ],\n",
       "       [0.84, 0.16],\n",
       "       [0.99, 0.01],\n",
       "       [0.82, 0.18],\n",
       "       [0.07, 0.93],\n",
       "       [0.96, 0.04],\n",
       "       [0.02, 0.98],\n",
       "       [0.93, 0.07],\n",
       "       [0.  , 1.  ],\n",
       "       [0.23, 0.77],\n",
       "       [0.99, 0.01],\n",
       "       [1.  , 0.  ],\n",
       "       [0.8 , 0.2 ],\n",
       "       [0.11, 0.89],\n",
       "       [0.09, 0.91],\n",
       "       [0.  , 1.  ],\n",
       "       [0.92, 0.08],\n",
       "       [0.23, 0.77],\n",
       "       [0.12, 0.88],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.15, 0.85],\n",
       "       [0.01, 0.99],\n",
       "       [1.  , 0.  ],\n",
       "       [0.17, 0.83],\n",
       "       [0.88, 0.12],\n",
       "       [0.14, 0.86],\n",
       "       [0.61, 0.39],\n",
       "       [0.26, 0.74],\n",
       "       [0.16, 0.84],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.91, 0.09],\n",
       "       [0.  , 1.  ],\n",
       "       [0.63, 0.37],\n",
       "       [0.86, 0.14],\n",
       "       [1.  , 0.  ],\n",
       "       [0.15, 0.85],\n",
       "       [0.8 , 0.2 ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.15, 0.85],\n",
       "       [0.12, 0.88],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.72, 0.28],\n",
       "       [1.  , 0.  ],\n",
       "       [0.15, 0.85],\n",
       "       [0.68, 0.32],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.15, 0.85],\n",
       "       [0.01, 0.99],\n",
       "       [0.87, 0.13],\n",
       "       [0.84, 0.16],\n",
       "       [0.71, 0.29],\n",
       "       [1.  , 0.  ],\n",
       "       [0.04, 0.96],\n",
       "       [1.  , 0.  ],\n",
       "       [0.15, 0.85],\n",
       "       [0.13, 0.87],\n",
       "       [0.28, 0.72],\n",
       "       [0.11, 0.89],\n",
       "       [0.25, 0.75],\n",
       "       [0.93, 0.07],\n",
       "       [0.01, 0.99],\n",
       "       [1.  , 0.  ],\n",
       "       [0.72, 0.28],\n",
       "       [0.98, 0.02],\n",
       "       [0.88, 0.12],\n",
       "       [0.04, 0.96],\n",
       "       [0.87, 0.13],\n",
       "       [0.11, 0.89],\n",
       "       [0.02, 0.98],\n",
       "       [1.  , 0.  ],\n",
       "       [0.96, 0.04],\n",
       "       [0.01, 0.99],\n",
       "       [0.11, 0.89],\n",
       "       [1.  , 0.  ],\n",
       "       [0.91, 0.09],\n",
       "       [0.11, 0.89],\n",
       "       [0.13, 0.87],\n",
       "       [1.  , 0.  ],\n",
       "       [0.98, 0.02],\n",
       "       [1.  , 0.  ],\n",
       "       [0.23, 0.77],\n",
       "       [0.  , 1.  ],\n",
       "       [0.1 , 0.9 ],\n",
       "       [0.15, 0.85],\n",
       "       [0.85, 0.15],\n",
       "       [0.  , 1.  ],\n",
       "       [0.15, 0.85],\n",
       "       [0.  , 1.  ],\n",
       "       [0.92, 0.08],\n",
       "       [0.93, 0.07],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.89, 0.11],\n",
       "       [0.01, 0.99],\n",
       "       [0.11, 0.89],\n",
       "       [0.15, 0.85],\n",
       "       [0.15, 0.85],\n",
       "       [1.  , 0.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.92, 0.08],\n",
       "       [0.15, 0.85],\n",
       "       [0.15, 0.85],\n",
       "       [0.09, 0.91],\n",
       "       [0.26, 0.74],\n",
       "       [0.  , 1.  ],\n",
       "       [0.22, 0.78],\n",
       "       [0.11, 0.89],\n",
       "       [1.  , 0.  ],\n",
       "       [0.25, 0.75],\n",
       "       [0.75, 0.25],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.48, 0.52],\n",
       "       [0.92, 0.08],\n",
       "       [0.84, 0.16],\n",
       "       [0.15, 0.85],\n",
       "       [0.17, 0.83],\n",
       "       [0.14, 0.86],\n",
       "       [0.9 , 0.1 ],\n",
       "       [0.01, 0.99],\n",
       "       [0.18, 0.82],\n",
       "       [0.15, 0.85],\n",
       "       [0.11, 0.89],\n",
       "       [0.03, 0.97],\n",
       "       [0.99, 0.01],\n",
       "       [0.11, 0.89],\n",
       "       [0.92, 0.08],\n",
       "       [0.  , 1.  ],\n",
       "       [0.18, 0.82],\n",
       "       [0.15, 0.85],\n",
       "       [0.79, 0.21],\n",
       "       [0.89, 0.11],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.26, 0.74],\n",
       "       [0.95, 0.05],\n",
       "       [0.47, 0.53],\n",
       "       [0.83, 0.17],\n",
       "       [0.63, 0.37],\n",
       "       [0.02, 0.98],\n",
       "       [1.  , 0.  ],\n",
       "       [0.99, 0.01],\n",
       "       [0.93, 0.07],\n",
       "       [0.94, 0.06],\n",
       "       [0.93, 0.07],\n",
       "       [1.  , 0.  ],\n",
       "       [0.96, 0.04],\n",
       "       [0.28, 0.72],\n",
       "       [0.05, 0.95],\n",
       "       [1.  , 0.  ],\n",
       "       [0.15, 0.85],\n",
       "       [0.1 , 0.9 ],\n",
       "       [0.08, 0.92],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.89, 0.11],\n",
       "       [0.11, 0.89],\n",
       "       [0.1 , 0.9 ],\n",
       "       [0.12, 0.88],\n",
       "       [1.  , 0.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.  , 1.  ],\n",
       "       [0.32, 0.68]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Alternatively, we can get only the second column\r\n",
    "# The main idea is that we are often interested in ONLY ONE of the two outcomes\r\n",
    "# In this case we would like to know if the customer will convert again\r\n",
    "# Once more, we can round to 0 digits, to achieve only 0s or 1s\r\n",
    "model.predict(test_inputs)[:,1].round(0)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 1.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# A much better approach here would be to use argmax (arguments of the maxima)\r\n",
    "# Argmax indicates the position of the highest argument row-wise or column-wise\r\n",
    "# In our case, we want ot know which COLUMN has the higher argument (probability), therefore we set axis=1 (for columns)\r\n",
    "# The output would be the column ID with the highest argument for each observation (row)\r\n",
    "# For instance, the first observation (in our output) was [0.93,0.07]\r\n",
    "# np.argmax([0.93,0.07], axis=1) would find that 0.91 is the higher argument (higher probability) and return 0\r\n",
    "# This method is great for multi-class problems as it is independent of the number of classes\r\n",
    "np.argmax(model.predict(test_inputs),axis=1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Finally we save the model using the built-in method TensorFlow method\r\n",
    "# We choose the name and the file extension\r\n",
    "# Since the HDF format is optimal for large numerical objects, that's our preferred choice here (and the one recommended by TF)\r\n",
    "# The proper extension is .h5 to indicate HDF, version 5\r\n",
    "model.save('audiobooks_model.h5') "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('hashanalytics': conda)"
  },
  "interpreter": {
   "hash": "fda10c17a9df46d77c0ae753b4e3a2aa8c45f7d9584953950091d4d531a44370"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}